{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Baseline code\n",
    "\n",
    "[1] https://www.kaggle.com/ahmedaffan789/bert-toxic-comments-pytorch\n",
    "\n",
    "#### Text Normalization\n",
    "\n",
    "[2] https://gist.github.com/seanh/0a56cd528714496625662dd9136d0cd3\n",
    "\n",
    "[4] https://towardsdatascience.com/text-normalization-for-natural-language-processing-nlp-70a314bfa646\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>Text_data</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_pid_1</td>\n",
       "      <td>Waiting for my mind to have a breakdown once t...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_pid_2</td>\n",
       "      <td>My new years resolution : I'm gonna get my ass...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_pid_3</td>\n",
       "      <td>New year : Somone else Feeling like 2020 will ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_pid_4</td>\n",
       "      <td>My story I guess : Hi, Im from Germany and my ...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_pid_5</td>\n",
       "      <td>Sat in the dark and cried myself going into th...</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8886</th>\n",
       "      <td>train_pid_8887</td>\n",
       "      <td>Ways to reverse memory loss from depression? :...</td>\n",
       "      <td>severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8887</th>\n",
       "      <td>train_pid_8888</td>\n",
       "      <td>A Comprehensive Guide To Slowly Getting Better...</td>\n",
       "      <td>severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8888</th>\n",
       "      <td>train_pid_8889</td>\n",
       "      <td>I don’t think college is right for me : TW: su...</td>\n",
       "      <td>severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8889</th>\n",
       "      <td>train_pid_8890</td>\n",
       "      <td>Please help: Severe insomnia affecting me in m...</td>\n",
       "      <td>severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8890</th>\n",
       "      <td>train_pid_8891</td>\n",
       "      <td>With each passing day my depression is getting...</td>\n",
       "      <td>severe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8891 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PID                                          Text_data  \\\n",
       "0        train_pid_1  Waiting for my mind to have a breakdown once t...   \n",
       "1        train_pid_2  My new years resolution : I'm gonna get my ass...   \n",
       "2        train_pid_3  New year : Somone else Feeling like 2020 will ...   \n",
       "3        train_pid_4  My story I guess : Hi, Im from Germany and my ...   \n",
       "4        train_pid_5  Sat in the dark and cried myself going into th...   \n",
       "...              ...                                                ...   \n",
       "8886  train_pid_8887  Ways to reverse memory loss from depression? :...   \n",
       "8887  train_pid_8888  A Comprehensive Guide To Slowly Getting Better...   \n",
       "8888  train_pid_8889  I don’t think college is right for me : TW: su...   \n",
       "8889  train_pid_8890  Please help: Severe insomnia affecting me in m...   \n",
       "8890  train_pid_8891  With each passing day my depression is getting...   \n",
       "\n",
       "         Label  \n",
       "0     moderate  \n",
       "1     moderate  \n",
       "2     moderate  \n",
       "3     moderate  \n",
       "4     moderate  \n",
       "...        ...  \n",
       "8886    severe  \n",
       "8887    severe  \n",
       "8888    severe  \n",
       "8889    severe  \n",
       "8890    severe  \n",
       "\n",
       "[8891 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "train_df = pd.read_csv('dataset/train.tsv', sep='\\t')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.3, random_state=2020)\n",
    "train_df.to_csv('dataset/train_80.tsv', sep='\\t', index=False)\n",
    "val_df.to_csv('dataset/dev_20.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.countplot(train_df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up sampling\n",
    "severe_df = train_df[train_df.Label == 'severe'].sample(3000, replace=True)\n",
    "\n",
    "ndepress_df = train_df[train_df.Label == 'not depression'].sample(3000, replace=True)\n",
    "\n",
    "moderate_df = train_df[train_df.Label == 'moderate'].sample(3000, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([moderate_df, ndepress_df, severe_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('dataset/train_upsampling.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/congvm/miniconda3/lib/python3.8/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Label', ylabel='count'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"395.328125pt\" height=\"262.19625pt\" viewBox=\"0 0 395.328125 262.19625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-13T16:54:54.778780</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.5.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 395.328125 262.19625 \nL 395.328125 0 \nL 0 0 \nL 0 262.19625 \nz\n\" style=\"fill: none\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 53.328125 224.64 \nL 388.128125 224.64 \nL 388.128125 7.2 \nL 53.328125 7.2 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 64.488125 224.64 \nL 153.768125 224.64 \nL 153.768125 17.554286 \nL 64.488125 17.554286 \nz\n\" clip-path=\"url(#p4f45dce9bc)\" style=\"fill: #3274a1\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 176.088125 224.64 \nL 265.368125 224.64 \nL 265.368125 17.554286 \nL 176.088125 17.554286 \nz\n\" clip-path=\"url(#p4f45dce9bc)\" style=\"fill: #e1812c\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 287.688125 224.64 \nL 376.968125 224.64 \nL 376.968125 17.554286 \nL 287.688125 17.554286 \nz\n\" clip-path=\"url(#p4f45dce9bc)\" style=\"fill: #3a923a\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"m5ae616d09f\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m5ae616d09f\" x=\"109.128125\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(105.946875 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#m5ae616d09f\" x=\"220.728125\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(217.546875 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#m5ae616d09f\" x=\"332.328125\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(329.146875 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_4\">\n     <!-- Label -->\n     <g transform=\"translate(207.238281 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\nM 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2969 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use xlink:href=\"#DejaVuSans-61\" x=\"55.712891\"/>\n      <use xlink:href=\"#DejaVuSans-62\" x=\"116.992188\"/>\n      <use xlink:href=\"#DejaVuSans-65\" x=\"180.46875\"/>\n      <use xlink:href=\"#DejaVuSans-6c\" x=\"241.992188\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_4\">\n      <defs>\n       <path id=\"m8888b2e791\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m8888b2e791\" x=\"53.328125\" y=\"224.64\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 0 -->\n      <g transform=\"translate(39.965625 228.439219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#m8888b2e791\" x=\"53.328125\" y=\"190.125714\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(27.240625 193.924933)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#m8888b2e791\" x=\"53.328125\" y=\"155.611429\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1000 -->\n      <g transform=\"translate(20.878125 159.410647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_7\">\n      <g>\n       <use xlink:href=\"#m8888b2e791\" x=\"53.328125\" y=\"121.097143\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1500 -->\n      <g transform=\"translate(20.878125 124.896362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m8888b2e791\" x=\"53.328125\" y=\"86.582857\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2000 -->\n      <g transform=\"translate(20.878125 90.382076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m8888b2e791\" x=\"53.328125\" y=\"52.068571\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 2500 -->\n      <g transform=\"translate(20.878125 55.86779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m8888b2e791\" x=\"53.328125\" y=\"17.554286\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 3000 -->\n      <g transform=\"translate(20.878125 21.353504)scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-33\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"190.869141\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- count -->\n     <g transform=\"translate(14.798438 130.02625)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-75\" d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-63\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"54.980469\"/>\n      <use xlink:href=\"#DejaVuSans-75\" x=\"116.162109\"/>\n      <use xlink:href=\"#DejaVuSans-6e\" x=\"179.541016\"/>\n      <use xlink:href=\"#DejaVuSans-74\" x=\"242.919922\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 53.328125 224.64 \nL 53.328125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_7\">\n    <path d=\"M 388.128125 224.64 \nL 388.128125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_8\">\n    <path d=\"M 53.328125 224.64 \nL 388.128125 224.64 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_9\">\n    <path d=\"M 53.328125 7.2 \nL 388.128125 7.2 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p4f45dce9bc\">\n   <rect x=\"53.328125\" y=\"7.2\" width=\"334.8\" height=\"217.44\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('dataset/train_prepr.tsv', sep='\\t')\n",
    "sns.countplot(df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# NLP\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Random Seed Initialize\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "# Device Optimization\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"y'know\": \"you know\",\n",
    "    \"ain't\": \"are not\",\n",
    "    \"'s\": \" is\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'll've\": \"i will have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"cant\": \"can not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"gotta\": \"have to\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who're\": \"who are\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where're\": \"where are\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when're\": \"when are\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"how're\": \"how are\",\n",
    "    \"I'm\": \"i am\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"there're\": \"there are\",\n",
    "    \"I've\": \"i have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"not've\": \"not have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Text preprocessing steps include a few essential tasks to further clean the available text data. It includes tasks like:-\n",
    "\n",
    "1. Stop-Word Removal : In English words like a, an, the, as, in, on, etc. are considered as stop-words so according to our requirements we can remove them to reduce vocabulary size as these words don't have some specific meaning\n",
    "\n",
    "2. Lower Casing : Convert all words into the lower case because the upper or lower case may not make a difference for the problem. And we are reducing vocabulary size by doing so.\n",
    "\n",
    "3. Stemming : Stemming refers to the process of removing suffixes and reducing a word to some base form such that all different variants of that word can be represented by the same form (e.g., “walk” and “walking” are both reduced to “walk”).\n",
    "\n",
    "4. Tokenization : NLP software typically analyzes text by breaking it up into words (tokens) and sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'no' in stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "# Why \"not\" a stopword\n",
    "# https://datascience.stackexchange.com/questions/15765/nlp-why-is-not-a-stop-word\n",
    "# Remove 'not' from stopwords\n",
    "keep_words = ['not', 'no']\n",
    "for w in keep_words:\n",
    "    stopwords.remove(w)\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str, stopwords: list):\n",
    "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "    text = pattern.sub('', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_website_links(text):\n",
    "    template = re.compile(r'https?://\\S+|www\\.\\S+')  # Removes website links\n",
    "    text = template.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_html_tags(text: str):\n",
    "    soup = BeautifulSoup(text, 'html.parser')  # Removes HTML tags\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def remove_emoji(text: str):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def expand_contractions(s, contractions_dict: dict = contractions_dict):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "\n",
    "def text_cleaning(paragraph):\n",
    "    '''\n",
    "    Cleans text into a basic form for NLP. Operations include the following:-\n",
    "    0. Lower case\n",
    "    1. Remove special charecters like &, #, etc\n",
    "    2. Removes extra spaces\n",
    "    3. Removes embedded URL links\n",
    "    4. Removes HTML tags\n",
    "    5. Removes emojis\n",
    "    \n",
    "    text - Text piece to be cleaned.\n",
    "    '''\n",
    "\n",
    "    paragraph = paragraph.lower()\n",
    "    paragraph = re.sub(r\"’\", \"'\", paragraph)\n",
    "    paragraph = remove_website_links(paragraph)\n",
    "    paragraph = remove_html_tags(paragraph)\n",
    "    paragraph = remove_emoji(paragraph)\n",
    "\n",
    "    # paragraph = paragraph.replace('.', ' .')\n",
    "    # paragraph = re.sub(r\"[^...]\", \" \", paragraph)\n",
    "    # paragraph = re.sub(r\"[^a-zA-Z\\d]\", \" \", paragraph)  # Remove special Charecters\n",
    "    sents = nltk.sent_tokenize(paragraph)\n",
    "    # paragraph = paragraph.replace(' .', '.')\n",
    "    \n",
    "    # return sents\n",
    "\n",
    "    for i in range(len(sents)):\n",
    "        sent = sents[i]\n",
    "        sent = expand_contractions(sent, contractions_dict)\n",
    "        sent = remove_stopwords(sent, stopwords)\n",
    "        sent = re.sub(r\"[^a-zA-Z\\d]\", \" \", sent)  # Remove special Charecters\n",
    "        sent = re.sub(' +', ' ', sent)  # Remove Extra Spaces\n",
    "        sent = sent.strip()\n",
    "        sents[i] = sent\n",
    "\n",
    "    paragraph = ' [SEP] '.join(sents)\n",
    "    # paragraph = '<s> ' + paragraph + ' </s>'\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0].Text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaning(train_df.iloc[0].Text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.sent_tokenize('last year, i tried not to have any breakdowns for the start of 2019 .3 .4 . a mere 10 days later, i broke down crying.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "train_df.Text_data = train_df.Text_data.progress_apply(text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(train_df.Text_data.apply(lambda x: len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = plt.hist(train_df.Text_data.apply(lambda x: len(x.split())), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train_df.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "# Load dataset\n",
    "train_df = pd.read_csv('dataset/train.tsv', sep='\\t')\n",
    "dev_df = pd.read_csv('dataset/dev_with_labels.tsv', sep='\\t')\n",
    "dev_df = dev_df.rename(columns={'Text data': 'Text_data'})\n",
    "\n",
    "train_df.Text_data = train_df.Text_data.progress_apply(text_cleaning)\n",
    "dev_df.Text_data = dev_df.Text_data.progress_apply(text_cleaning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'device': device,\n",
    "    'debug': False,\n",
    "    'checkpoint': 'bert-base-uncased',  #'allenai/longformer-base-4096',\n",
    "    'output_logits': 768,\n",
    "    'max_len': 512,\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.2,\n",
    "    'num_workers': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LongformerModel, LongformerTokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(params['checkpoint'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.Text_data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer.encode(train_df.Text_data.iloc[0])\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEDIDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 text_df: pd.DataFrame,\n",
    "                 max_len: int = params['max_len'],\n",
    "                 checkpoint: str = params['checkpoint'],\n",
    "                 is_training=True):\n",
    "        super().__init__()\n",
    "        self.is_training = is_training\n",
    "        self.text_df = text_df\n",
    "        self.max_len = max_len\n",
    "        self.checkpoint = checkpoint\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(checkpoint)\n",
    "        self.label_mapping = {'moderate': 0, 'not depression': 1, 'severe': 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.text_df.iloc[idx].Text_data)\n",
    "\n",
    "        tokenized_text = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized_text['input_ids']\n",
    "        attention_mask = tokenized_text['attention_mask']\n",
    "        token_type_ids = tokenized_text['token_type_ids']\n",
    "\n",
    "        if self.is_training:\n",
    "            label = self.label_mapping[str(self.text_df.iloc[idx].Label)]\n",
    "            return {\n",
    "                'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask,\n",
    "                                               dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids,\n",
    "                                               dtype=torch.long),\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_mask,\n",
    "                                               dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids,\n",
    "                                               dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextEDIDataset(train_df)\n",
    "dev_dataset = TextEDIDataset(dev_df)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=params['batch_size'],\n",
    "                          num_workers=4)\n",
    "dev_loader = DataLoader(dev_dataset,\n",
    "                          batch_size=params['batch_size'],\n",
    "                          num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerModel\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "class DepressionModel(nn.Module):\n",
    "    def __init__(self, checkpoint=params['checkpoint'], params=params):\n",
    "        super(DepressionModel, self).__init__()\n",
    "        self.checkpoint = checkpoint\n",
    "        # self.bert = LongformerModel.from_pretrained(checkpoint, return_dict=False)\n",
    "        self.model = BertModel.from_pretrained(checkpoint,\n",
    "                                               gradient_checkpointing=True)\n",
    "        self.layer_norm = nn.LayerNorm(params['output_logits'])\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        self.dense = nn.Sequential(nn.Linear(params['output_logits'], 256),\n",
    "                                   nn.LeakyReLU(negative_slope=0.01),\n",
    "                                   nn.Dropout(params['dropout']),\n",
    "                                   nn.Linear(256, 3))\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        _, pooled_output = self.model(input_ids=input_ids,\n",
    "                                      token_type_ids=token_type_ids,\n",
    "                                      attention_mask=attention_mask)\n",
    "        pooled_output = self.layer_norm(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        preds = self.dense(pooled_output)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DepressionModel()\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from collections import OrderedDict\n",
    "from transformers import AdamW\n",
    "\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLightning(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = DepressionModel()\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_optimizer = list(self.model.named_parameters())\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer\n",
    "                    if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\":\n",
    "                0.01\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer\n",
    "                    if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\":\n",
    "                0.0\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=2e-5,\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "\n",
    "        logits = self.model(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        loss = self.criterion(logits, labels)\n",
    "        tqdm_dict = {\"train_loss\": loss}\n",
    "        output = OrderedDict({\n",
    "            \"loss\": loss,\n",
    "            \"progress_bar\": tqdm_dict,\n",
    "            \"log\": tqdm_dict\n",
    "        })\n",
    "\n",
    "        return output\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "\n",
    "        logits = self.model(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        loss = self.criterion(logits, labels)\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct_count = torch.sum(labels == labels_hat)\n",
    "\n",
    "        if self.on_gpu:\n",
    "            correct_count = correct_count.cuda(loss.device.index)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            \"val_loss\": loss,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"batch_size\": len(labels)\n",
    "        })\n",
    "        return output\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        val_acc = sum([out[\"correct_count\"]\n",
    "                       for out in outputs]).float() / sum(out[\"batch_size\"]\n",
    "                                                          for out in outputs)\n",
    "        val_loss = sum([out[\"val_loss\"] for out in outputs]) / len(outputs)\n",
    "        tqdm_dict = {\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc,\n",
    "        }\n",
    "        result = {\n",
    "            \"progress_bar\": tqdm_dict,\n",
    "            \"log\": tqdm_dict,\n",
    "            \"val_loss\": val_loss\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        token_type_ids = batch[\"token_type_ids\"]\n",
    "\n",
    "        loss, logits = self.model(input_ids,\n",
    "                                  token_type_ids=token_type_ids,\n",
    "                                  attention_mask=attention_mask,\n",
    "                                  labels=labels)\n",
    "        labels_hat = torch.argmax(logits, dim=1)\n",
    "\n",
    "        correct_count = torch.sum(labels == labels_hat)\n",
    "\n",
    "        if self.on_gpu:\n",
    "            correct_count = correct_count.cuda(loss.device.index)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            \"test_loss\": loss,\n",
    "            \"correct_count\": correct_count,\n",
    "            \"batch_size\": len(labels)\n",
    "        })\n",
    "\n",
    "        return output\n",
    "\n",
    "    def test_end(self, outputs):\n",
    "        test_acc = sum([out[\"correct_count\"]\n",
    "                        for out in outputs]).float() / sum(out[\"batch_size\"]\n",
    "                                                           for out in outputs)\n",
    "        test_loss = sum([out[\"test_loss\"] for out in outputs]) / len(outputs)\n",
    "        tqdm_dict = {\n",
    "            \"test_loss\": test_loss,\n",
    "            \"test_acc\": test_acc,\n",
    "        }\n",
    "        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n",
    "                                    min_delta=0.0,\n",
    "                                    patience=3,\n",
    "                                    verbose=True,\n",
    "                                    mode=\"min\")\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, callbacks=[early_stop_callback])\n",
    "\n",
    "model_lightning = ModelLightning()\n",
    "\n",
    "trainer.fit(model_lightning,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=dev_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e332ef6ff1c3324623db12c5ca1ff1abfc9af7c1954ef7c96b2e14bf8a3515dc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
